{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "For the following exercises, use the training portion of the Brown corpus as defined by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "random.seed(0)\n",
    "tagged_sents = list(brown.tagged_sents())\n",
    "random.shuffle(tagged_sents)\n",
    "train = tagged_sents[:10000]\n",
    "test = tagged_sents[10000:10200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the training corpus is structured in this corpus: it is a list of training sentences, and each sentence is a list of tagged words. For example, the first sentence of the training set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Stars', 'NNS-HL'), ('for', 'IN-HL'), ('marriage', 'NN-HL')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find all labels and all vocabulary\n",
    "\n",
    "Using the training data (not the test data!), find the following information:\n",
    "\n",
    "1. What is the vocabulary size?\n",
    "2. What is the number of distinct lavels?\n",
    "3. What is the total number of words?\n",
    "4. What is the total number of labels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 23192\n",
      "Label set size is 329\n",
      "Number of words is 203375\n",
      "Number of labels is 203375\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find all word bigrams and all label bigrams\n",
    "\n",
    "Below we are going to find out statistics about what is the most likely word following another word, what is the most likely word or label beginning a sentence, and what is the most likely word or label ending a sentence. A common approach to do statistics on the first and last word or token is add a new word or token such as `$` at the beginning and the end of each sentence. This is what is called *padding*. For this exercise, generate the word and label bigrams after padding with the `$` symbol. For example, if the corpus is the pair of sentences:\n",
    "\n",
    "```\n",
    "[[('my','PRP$'),('sentence','NN')],[('my','PRP$'),('second','OD'),('sentence','NN')]]\n",
    "```\n",
    "\n",
    "Then the word bigrams are:\n",
    "\n",
    "```\n",
    "[('$','my'),('my','sentence'),('sentence','$'),('$','my'),('my','second'),('second','sentence'),('sentence','$')]\n",
    "```\n",
    "\n",
    "\n",
    "And the label bigrams are:\n",
    "\n",
    "```\n",
    "[('$','PRP$'),('PRP$','NN'),('NN','$'),('$','PRP$),('PRP$','OD'),('OD','NN'),('NN','$')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Most common words and labels beginning and ending a sentence\n",
    "\n",
    "1. What are the most common words beginning a sentence? \n",
    "2. What are the most common words ending a sentence?\n",
    "3. What are the most common PoS labels beginning a sentence?\n",
    "4. What are the most common PoS labels ending a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Most common PoS after another PoS\n",
    "\n",
    "Write a function `most_common_label(label, n)` that, when given a PoS label and a number n, it prints the n most common labels that follow the given label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AT', 1421), ('PPS', 1097), ('IN', 831), ('``', 763), ('RB', 681)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_label('$', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IN', 7362), ('.', 3499), (',', 3263)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_label('NN', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "The lectures of week 6 described several possible features to train a classifier for the task of named entity recognition. Implement feature extractors that focus on some of these features and train a Naive Bayes classifier. Use the training and test files from the CONLL 2002 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"conll2002\")\n",
    "from nltk.corpus import conll2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = conll2002.iob_sents('esp.train')\n",
    "test = conll2002.iob_sents('esp.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Distribution of IOB tags\n",
    "\n",
    "Calculate the distribution of IOB tags both in the train and the test set and confirm that the distributions are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Feature extractor\n",
    "\n",
    "Implement an NLTK feature extractor that extracts the following features of a word:\n",
    "\n",
    "* The Part of speech (this is the second element of the input data).\n",
    "* True if the first letter is a capital letter, False otherwise.\n",
    "* True if the word contains a digit, False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Classification\n",
    "\n",
    "Train a NLTK Naive Bayes classifier with the above features and test the accuracy on the test set. Compare the results with the system provided in the NER notebook of the lectures of week 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
